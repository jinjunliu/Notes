---
title: "Understanding Hadoop Core Components"
date: 2022-04-06
draft: false
weight: 604
---

## Java Virtual Machines (JVMs)

Hadoop processes run in seperate JVMs. JVMs don't share any state. Traditionally, in database processing systems, state is shared. JVM processes differ between Hadoop 1.0 and 2.0.

## HDFS and other file systems

Hadoop uses HDFS and data is triple replicated by default. HDFS has two modes of implementation - distributed and pseudo-distributed. Pseudo-distributed uses the HDFS system but is designed for testing and implemented in a single node on a single machine. 

An alternative to HDFS you can run Hadoop with the regular file system. This is called the standalone mode. It is a great way when you're just learning about the MapReduce paradigm. You are reducing the complexityby just working with you regular file system. 

Cloud file systems are common to use for storing data, such as Amazon S3, Google Cloud Storage, Azure Blob Storage, etc.

If you are deploying Hadoop in a single node, you are going to use the local file system and in a single JVM for all the Hadoop processes. If you deploy in pseudo-distributed mode, you are going to use HDFS and in a single JVM for all the Hadoop processes. If you run in fully distributed mode, you are going to use HDFS (triple replicated) and the demons are going to run in various locations depending where you choose to place them.

This is a drawing showing some of the common demons and how they are distributed. 

![a view of hadoop](/images/a_view_of_hadoop.png)

It is important to remember that the Hadoop processes are running in seperate JVMs. This means that the JVM is not sharing any state.

## Hadoop cluster components

![Hadoop Ecosystem](/images/hadoop_ecosystem.png)

