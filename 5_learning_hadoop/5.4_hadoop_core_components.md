---
title: "Understanding Hadoop Core Components"
date: 2022-04-06
draft: false
weight: 604
---

## Java Virtual Machines (JVMs)

Hadoop processes run in seperate JVMs. JVMs don't share any state. Traditionally, in database processing systems, state is shared. JVM processes differ between Hadoop 1.0 and 2.0.

## HDFS and other file systems

Hadoop uses HDFS and data is triple replicated by default. HDFS has two modes of implementation - distributed and pseudo-distributed. Pseudo-distributed uses the HDFS system but is designed for testing and implemented in a single node on a single machine. 

An alternative to HDFS you can run Hadoop with the regular file system. This is called the standalone mode. It is a great way when you're just learning about the MapReduce paradigm. You are reducing the complexityby just working with you regular file system. 

Cloud file systems are common to use for storing data, such as Amazon S3, Google Cloud Storage, Azure Blob Storage, etc.

If you are deploying Hadoop in a single node, you are going to use the local file system and in a single JVM for all the Hadoop processes. If you deploy in pseudo-distributed mode, you are going to use HDFS and in a single JVM for all the Hadoop processes. If you run in fully distributed mode, you are going to use HDFS (triple replicated) and the daemons are going to run in various locations depending where you choose to place them.

This is a drawing showing some of the common daemons and how they are distributed. 

![a view of hadoop](/images/a_view_of_hadoop.png)

It is important to remember that the Hadoop processes are running in seperate JVMs. This means that the JVM is not sharing any state.

## Hadoop cluster components

The following table are the components of Apache Hadoop ecosystem.

![Hadoop Ecosystem](/images/hadoop_ecosystem.png)

The following table shows the components of a commercial distribution - the Cloudera Hadoop ecosystem.

![Cloudera Hadoop](/images/cloudera_hadoop.png)

Hue is specific to Cloudera Hadoop. It is a graphical interface to the Hadoop ecosystem.

## Introducing Hadoop Spark

Apache Spark is for in-memory distributed data processing. It is initially developed at UC Berkeley for genomic analysis. Apache Spark has commercial distributions at Databricks. There are extensive Spark ecosystems, many libraries. Apache Spark is fast, general purpose processing engine. It is compatible with Hadoop data, clusters and YARN. You can process data in HDFS, HBase, Hive, and any InputFormat with Spark. Data can be processed via batch, streaming, interactively, and with Machine Learning libraries.

The following chart shows the components of the Spark ecosystem.

![Spark Ecosystem](/images/spark_ecosystem.png)

## Apache and Cloudera Hadoop distributions

Apache Hadoop main site: https://hadoop.apache.org/

Cloudera main site: https://www.cloudera.com/

## Public cloud to host Hadoop

The first thing we need to think about is are we going to use VMs or Docker containers to host Hadoop. We can use VM images and vendors can supply the images, we can scale via known patterns when using VMs. For using Docker, we need to build docker image from `Dockerfile`. Base image is the key. You must test to determine scaling patterns. 

Cloudera Hue Community Edition has had a lot of changes to their community edition and their web UI which is called Hue. Cloudera provides VM/Docker images to download. A Hue demo for learning purpose is available at [here](https://demo.gethue.com/).

However, using public cloud is a great way to host Hadoop. You can use GCP Dataproc, AWS EMR, Azure HDInsight, etc. GCP Dataproc only takes five minutes to spin up, and it is highly recommended for beginners.

## Quiz

1. Unlike the pseudo-distributed mode, the standalone mode _____. (C)
    * cannot be used in production
    * replicates each file three times
    * uses the regular file system
    * cannot be used for testing

2. If a Hadoop implementation uses multiple JVM daemons spread across several nodes, uses the HDFS filesystem, and stores every file in triplicate, which Hadoop mode is the implementation running? (A)
    * fully distributed
    * pseudo-distributed
    * single node
    * cloud managed

{{<hint info>}}
The code running on a real Hadoop cluster is conducted in this mode. It is the mode in which you see Hadoop's true power when you run your code on thousands of servers and against a massive input.
{{</hint>}}

3. Because Hadoop processes run in JVMs, they _____. (B)
    * are identical between all Hadoop versions
    * do not share state
    * only work in Hadoop version 2.0
    * share a state like other databases

4. You would like to quickly deploy a Hadoop testing cluster that uses real data, and that you will scale to production later. Which option is likely best for this task? (A)
    * using a GCP Dataproc cluster
    * creating your own cluster using Docker
    * building your own cluster from scratch
    * using Cloudera Hue Community Edition

5. When setting up a Hadoop cluster, which configuration is simplest to scale and can use a vendor-provided image for minimal setup with technical support? (A)
    * virtual machines on public cloud
    * docker containers on bare metal
    * virtual machines on bare metal
    * docker containers on cloud

{{<hint info>}}
When using public cloud vendors to run full virtual machines, the VM image that is most compatible is often provided by the vendor and is manageable through a simple interface, often with options for contacting technical support.
{{</hint>}}

6. Which Cloudera product allows you to try some Hadoop functions in a browser? (C)
    * Cloudera Express
    * Cloudera Enterprise
    * Cloudera Live
    * Cloudera Director

7. Which is true about Apache Spark? (B)
    * It supports HBase but not HDFS.
    * It supports both Java and Python.
    * It uses the disk for processing.
    * It is only available as a commercial solution.

8. What kind of task is Apache Spark well suited to? (A)
    * distributed processing of very large datasets
    * version management of Hadoop modules
    * consistent querying of transactional databases
    * long term storage of large databases

{{<hint info>}}
Apache Spark is a distributed, open-source data workload processing system that uses in-memory caching and streamlined query execution to easily query any size data and is simply a fast, general engine for the processing of large-scale data.
{{</hint>}}

9. Which component in the Hadoop ecosystem is used for predictive analytics? (A)
    * Mahout
    * Sqoop
    * Flume
    * YARN

10. What language library is used to query Hbase?
    * mahout
    * pig
    * oozie
    * HQL

{{<hint info>}}
Hive offers a method for projecting a structure onto the Hadoop data and for querying the data using the HiveQL (HQL) language using HBase.
{{</hint>}}

11. What is the most important difference between Java Virtual Machines (JVMs) and traditional database processing systems? (B)
    * Different Hadoop processes can run in the same JVM.
    * JVMs do not share state.
    * JVMs cannot perform batch processing.
    * JVM processes are exactly the same between Hadoop 1.0 and 2.0.

{{<hint info>}}
Unlike traditional relational database management systems, JVMs do not share state.
{{</hint>}}
